{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9612f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from tbselenium.tbdriver import TorBrowserDriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "694fa8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://vice2e3gr3pmaikukidllstulxvkb7a247gkguihzvyk3gqwdpolqead.onion/'\n",
    "USERNAME = 'jheronimus'\n",
    "PASSWORD = 'dataforensics'\n",
    "\n",
    "\n",
    "def visit_and_login(driver):\n",
    "    '''This function opens the marketplace in the url'''\n",
    "    \n",
    "    # Log in and solve the captcha\n",
    "    driver.get(URL)\n",
    "    time.sleep(65) #60 seconds to solve the captcha and login\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "def get_pages(driver, urls):\n",
    "    print(\"Starting the crawling\")\n",
    "    \n",
    "    driver.get(URL)\n",
    "    \n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    hrefs = soup.find_all(href=True) #find all linked urls\n",
    "    for line in hrefs:\n",
    "        urls.append(line['href'])\n",
    "    \n",
    "#     filename = 'vice_city/page1.html'\n",
    "#     with open(\"/home/levi/\" + filename, \"w\") as f:\n",
    "#             f.write(driver.page_source)\n",
    "    \n",
    "    for i in range(2, 181):\n",
    "        site_page = URL +'?category=0&pg=' + str(i)\n",
    "        driver.get(site_page)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        \n",
    "        soup = bs(driver.page_source, 'html.parser')\n",
    "        hrefs = soup.find_all(href=True) #find all linked urls\n",
    "        for line in hrefs:\n",
    "            urls.append(line['href'])\n",
    "        \n",
    "#         filename = 'vice_city/'+ 'page'+str(i) + '.html'\n",
    "#         with open(\"/home/levi/\" + filename, \"w\") as f:\n",
    "#             f.write(driver.page_source)\n",
    "        print('crawled',i, 'pages')\n",
    "    print('Finished Crawling')\n",
    "    return urls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9db6ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dfc67a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the crawling\n",
      "crawled 2 page\n",
      "crawled 3 page\n",
      "crawled 4 page\n",
      "crawled 5 page\n",
      "crawled 6 page\n",
      "crawled 7 page\n",
      "crawled 8 page\n",
      "crawled 9 page\n",
      "crawled 10 page\n",
      "crawled 11 page\n",
      "crawled 12 page\n",
      "crawled 13 page\n",
      "crawled 14 page\n",
      "crawled 15 page\n",
      "crawled 16 page\n",
      "crawled 17 page\n",
      "crawled 18 page\n",
      "crawled 19 page\n",
      "crawled 20 page\n",
      "crawled 21 page\n",
      "crawled 22 page\n",
      "crawled 23 page\n",
      "crawled 24 page\n",
      "crawled 25 page\n",
      "crawled 26 page\n",
      "crawled 27 page\n",
      "crawled 28 page\n",
      "crawled 29 page\n",
      "crawled 30 page\n",
      "crawled 31 page\n",
      "crawled 32 page\n",
      "crawled 33 page\n",
      "crawled 34 page\n",
      "crawled 35 page\n",
      "crawled 36 page\n",
      "crawled 37 page\n",
      "crawled 38 page\n",
      "crawled 39 page\n",
      "crawled 40 page\n",
      "crawled 41 page\n",
      "crawled 42 page\n",
      "crawled 43 page\n",
      "crawled 44 page\n",
      "crawled 45 page\n",
      "crawled 46 page\n",
      "crawled 47 page\n",
      "crawled 48 page\n",
      "crawled 49 page\n",
      "crawled 50 page\n",
      "crawled 51 page\n",
      "crawled 52 page\n",
      "crawled 53 page\n",
      "crawled 54 page\n",
      "crawled 55 page\n",
      "crawled 56 page\n",
      "crawled 57 page\n",
      "crawled 58 page\n",
      "crawled 59 page\n",
      "crawled 60 page\n",
      "crawled 61 page\n",
      "crawled 62 page\n",
      "crawled 63 page\n",
      "crawled 64 page\n",
      "crawled 65 page\n",
      "crawled 66 page\n",
      "crawled 67 page\n",
      "crawled 68 page\n",
      "crawled 69 page\n",
      "crawled 70 page\n",
      "crawled 71 page\n",
      "crawled 72 page\n",
      "crawled 73 page\n",
      "crawled 74 page\n",
      "crawled 75 page\n",
      "crawled 76 page\n",
      "crawled 77 page\n",
      "crawled 78 page\n",
      "crawled 79 page\n",
      "crawled 80 page\n",
      "crawled 81 page\n",
      "crawled 82 page\n",
      "crawled 83 page\n",
      "crawled 84 page\n",
      "crawled 85 page\n",
      "crawled 86 page\n",
      "crawled 87 page\n",
      "crawled 88 page\n",
      "crawled 89 page\n",
      "crawled 90 page\n",
      "crawled 91 page\n",
      "crawled 92 page\n",
      "crawled 93 page\n",
      "crawled 94 page\n",
      "crawled 95 page\n",
      "crawled 96 page\n",
      "crawled 97 page\n",
      "crawled 98 page\n",
      "crawled 99 page\n",
      "crawled 100 page\n",
      "crawled 101 page\n",
      "crawled 102 page\n",
      "crawled 103 page\n",
      "crawled 104 page\n",
      "crawled 105 page\n",
      "crawled 106 page\n",
      "crawled 107 page\n",
      "crawled 108 page\n",
      "crawled 109 page\n",
      "crawled 110 page\n",
      "crawled 111 page\n",
      "crawled 112 page\n",
      "crawled 113 page\n",
      "crawled 114 page\n",
      "crawled 115 page\n",
      "crawled 116 page\n",
      "crawled 117 page\n",
      "crawled 118 page\n",
      "crawled 119 page\n",
      "crawled 120 page\n",
      "crawled 121 page\n",
      "crawled 122 page\n",
      "crawled 123 page\n",
      "crawled 124 page\n",
      "crawled 125 page\n",
      "crawled 126 page\n",
      "crawled 127 page\n",
      "crawled 128 page\n",
      "crawled 129 page\n",
      "crawled 130 page\n",
      "crawled 131 page\n",
      "crawled 132 page\n",
      "crawled 133 page\n",
      "crawled 134 page\n",
      "crawled 135 page\n",
      "crawled 136 page\n",
      "crawled 137 page\n",
      "crawled 138 page\n",
      "crawled 139 page\n",
      "crawled 140 page\n",
      "crawled 141 page\n",
      "crawled 142 page\n",
      "crawled 143 page\n",
      "crawled 144 page\n",
      "crawled 145 page\n",
      "crawled 146 page\n",
      "crawled 147 page\n",
      "crawled 148 page\n",
      "crawled 149 page\n",
      "crawled 150 page\n",
      "crawled 151 page\n",
      "crawled 152 page\n",
      "crawled 153 page\n",
      "crawled 154 page\n",
      "crawled 155 page\n",
      "crawled 156 page\n",
      "crawled 157 page\n",
      "crawled 158 page\n",
      "crawled 159 page\n",
      "crawled 160 page\n",
      "crawled 161 page\n",
      "crawled 162 page\n",
      "crawled 163 page\n",
      "crawled 164 page\n",
      "crawled 165 page\n",
      "crawled 166 page\n",
      "crawled 167 page\n",
      "crawled 168 page\n",
      "crawled 169 page\n",
      "crawled 170 page\n",
      "crawled 171 page\n",
      "crawled 172 page\n",
      "crawled 173 page\n",
      "crawled 174 page\n",
      "crawled 175 page\n",
      "crawled 176 page\n",
      "crawled 177 page\n",
      "crawled 178 page\n",
      "crawled 179 page\n",
      "crawled 180 page\n",
      "Finished Crawling\n"
     ]
    }
   ],
   "source": [
    "with TorBrowserDriver(\"/home/levi/tor-browser_en-US/\") as driver:\n",
    "    driver = visit_and_login(driver)\n",
    "    all_urls = get_pages(driver, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a8b0baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract the vendor urls from all the scraped urls\n",
    "vendor_urls = []\n",
    "for i in urls:\n",
    "    if 'user' in i and 'jheronimus' not in i:\n",
    "        vendor_urls.append(i)\n",
    "\n",
    "vendor_urls = list(dict.fromkeys(vendor_urls))#final list of vendor urls /remove duplicates\n",
    "vendor_urls = [URL + i for i in vendor_urls]\n",
    "\n",
    "import json\n",
    "with open(\"vice_city_urls.json\", \"w\") as f:\n",
    "    json.dump(vendor_urls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cabcec41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://vice2e3gr3pmaikukidllstulxvkb7a247gkguihzvyk3gqwdpolqead.onion/?page=profile&user=PsychedelicaShop'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vendor_urls[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6d912c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already crawled pages: 107\n",
      "pages left to crawl 82\n"
     ]
    }
   ],
   "source": [
    "#when crawling in multiple stages, this function updates the vendor_urls list\n",
    "\n",
    "def links_to_crawl(DIR, vendor_urls):\n",
    "    '''check in directory which pages already have been crawled and make new list with urls to crawl'''\n",
    "    \n",
    "    crawled = os.listdir(DIR)\n",
    "    crawled = [i[:-5] for i in crawled]\n",
    "    \n",
    "    print('already crawled pages:' , len(crawled))\n",
    "\n",
    "    all_vendors = [i[89:] for i in vendor_urls] #names of all vendors\n",
    "\n",
    "    #all vendors remaining to crawl\n",
    "    to_crawl = [vendor for vendor in all_vendors if vendor not in crawled]\n",
    "    site = URL+'?page=profile&user='\n",
    "\n",
    "    vendors_to_crawl = [site + vendor for vendor in to_crawl] #list of pages to crawl\n",
    "    print('pages left to crawl' , len(vendors_to_crawl))\n",
    "    \n",
    "    return vendors_to_crawl\n",
    "\n",
    "\n",
    "DIR = r'/home/levi/vice_city'\n",
    "vendor_urls = links_to_crawl(DIR, vendor_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a6fc1db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://vice2e3gr3pmaikukidllstulxvkb7a247gkguihzvyk3gqwdpolqead.onion/?page=profile&user=redlemon'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vendor_urls[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ee6f98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the crawling\n",
      "dowloaded 0 out of 82 pages\n",
      "dowloaded 1 out of 82 pages\n",
      "dowloaded 2 out of 82 pages\n",
      "dowloaded 3 out of 82 pages\n",
      "dowloaded 4 out of 82 pages\n",
      "dowloaded 5 out of 82 pages\n",
      "dowloaded 6 out of 82 pages\n",
      "dowloaded 7 out of 82 pages\n",
      "dowloaded 8 out of 82 pages\n",
      "dowloaded 9 out of 82 pages\n",
      "dowloaded 10 out of 82 pages\n",
      "dowloaded 11 out of 82 pages\n",
      "dowloaded 12 out of 82 pages\n",
      "dowloaded 13 out of 82 pages\n",
      "dowloaded 14 out of 82 pages\n",
      "dowloaded 15 out of 82 pages\n",
      "dowloaded 16 out of 82 pages\n",
      "dowloaded 17 out of 82 pages\n",
      "dowloaded 18 out of 82 pages\n",
      "dowloaded 19 out of 82 pages\n",
      "dowloaded 20 out of 82 pages\n",
      "dowloaded 21 out of 82 pages\n",
      "dowloaded 22 out of 82 pages\n",
      "dowloaded 23 out of 82 pages\n",
      "dowloaded 24 out of 82 pages\n",
      "dowloaded 25 out of 82 pages\n",
      "dowloaded 26 out of 82 pages\n",
      "dowloaded 27 out of 82 pages\n",
      "dowloaded 28 out of 82 pages\n",
      "dowloaded 29 out of 82 pages\n",
      "dowloaded 30 out of 82 pages\n",
      "dowloaded 31 out of 82 pages\n",
      "dowloaded 32 out of 82 pages\n",
      "dowloaded 33 out of 82 pages\n",
      "dowloaded 34 out of 82 pages\n",
      "dowloaded 35 out of 82 pages\n",
      "dowloaded 36 out of 82 pages\n",
      "dowloaded 37 out of 82 pages\n",
      "dowloaded 38 out of 82 pages\n",
      "dowloaded 39 out of 82 pages\n",
      "dowloaded 40 out of 82 pages\n",
      "dowloaded 41 out of 82 pages\n",
      "dowloaded 42 out of 82 pages\n",
      "dowloaded 43 out of 82 pages\n",
      "dowloaded 44 out of 82 pages\n",
      "dowloaded 45 out of 82 pages\n",
      "dowloaded 46 out of 82 pages\n",
      "dowloaded 47 out of 82 pages\n",
      "dowloaded 48 out of 82 pages\n",
      "dowloaded 49 out of 82 pages\n",
      "dowloaded 50 out of 82 pages\n",
      "dowloaded 51 out of 82 pages\n",
      "dowloaded 52 out of 82 pages\n",
      "dowloaded 53 out of 82 pages\n",
      "dowloaded 54 out of 82 pages\n",
      "dowloaded 55 out of 82 pages\n",
      "dowloaded 56 out of 82 pages\n",
      "dowloaded 57 out of 82 pages\n",
      "dowloaded 58 out of 82 pages\n",
      "dowloaded 59 out of 82 pages\n",
      "dowloaded 60 out of 82 pages\n",
      "dowloaded 61 out of 82 pages\n",
      "dowloaded 62 out of 82 pages\n",
      "dowloaded 63 out of 82 pages\n",
      "dowloaded 64 out of 82 pages\n",
      "dowloaded 65 out of 82 pages\n",
      "dowloaded 66 out of 82 pages\n",
      "dowloaded 67 out of 82 pages\n",
      "dowloaded 68 out of 82 pages\n",
      "dowloaded 69 out of 82 pages\n",
      "dowloaded 70 out of 82 pages\n",
      "dowloaded 71 out of 82 pages\n",
      "dowloaded 72 out of 82 pages\n",
      "dowloaded 73 out of 82 pages\n",
      "dowloaded 74 out of 82 pages\n",
      "dowloaded 75 out of 82 pages\n",
      "dowloaded 76 out of 82 pages\n",
      "dowloaded 77 out of 82 pages\n",
      "dowloaded 78 out of 82 pages\n",
      "dowloaded 79 out of 82 pages\n",
      "dowloaded 80 out of 82 pages\n",
      "dowloaded 81 out of 82 pages\n"
     ]
    }
   ],
   "source": [
    "def get_vendor_pages(driver, vendor_urls):\n",
    "    print(\"Starting the crawling\")\n",
    "  \n",
    "    for index, i in enumerate(vendor_urls):\n",
    "        \n",
    "        site_page = i \n",
    "        driver.get(site_page)\n",
    "        time.sleep(2)\n",
    "\n",
    "        filename = 'vice_city/'+ str(i[89:]) + '.html' #save file als username\n",
    "        with open(\"/home/levi/\" + filename, \"w\") as f:\n",
    "            f.write(driver.page_source)\n",
    "            print('dowloaded' ,index, 'out of', len(vendor_urls),'pages')\n",
    "    print('finished crawling')\n",
    "    \n",
    "with TorBrowserDriver(\"/home/levi/tor-browser_en-US/\") as driver:\n",
    "    driver = visit_and_login(driver)\n",
    "    get_vendor_pages(driver, vendor_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe790d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

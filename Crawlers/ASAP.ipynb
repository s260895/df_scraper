{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74fdd5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from tbselenium.tbdriver import TorBrowserDriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ea1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL to visits\n",
    "URL = 'http://asap4u7rq4tyakf5gdahmj2c77blwc4noxnsppp5lzlhk7x34x2e22yd.onion/'\n",
    "\n",
    "\n",
    "def visit_and_login(driver):\n",
    "    '''This function opens the marketplace in the url'''\n",
    "    \n",
    "    # Log in and solve the captcha\n",
    "    driver.get(URL)\n",
    "    time.sleep(30) #30 seconds to solve the captcha and login\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "def get_pages(driver, urls):\n",
    "    print(\"Starting the crawling\")\n",
    "    \n",
    "    #visit all listings on the marketplace\n",
    "    for i in range(1, 239): #there are 238 pages\n",
    "        \n",
    "        site_page = URL +'search?page='+str(i)+'&currency=USD&sortType=None' #URL to visit\n",
    "        driver.get(site_page)#go to the url\n",
    "        time.sleep(2) #sleep for 2 seconds\n",
    "        \n",
    "        soup = bs(driver.page_source, 'html.parser') #view page source using BS4\n",
    "        hrefs = soup.find_all(href=True) #find all linked urls\n",
    "        \n",
    "        for line in hrefs:\n",
    "            urls.append(line['href']) #append all urls on the page to a list\n",
    "        \n",
    "        #print statement to show progress\n",
    "        if i%5 == 0:\n",
    "            print('crawled',i,'out of 239 pages')\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    print('Finished Crawling')\n",
    "    return urls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9f3da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will be the list of urls found at the page\n",
    "urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e42cc7b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TorBrowserDriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-58e5e1f949cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mTorBrowserDriver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/levi/tor-browser_en-US/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisit_and_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mall_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TorBrowserDriver' is not defined"
     ]
    }
   ],
   "source": [
    "#crawling\n",
    "with TorBrowserDriver(\"/home/levi/tor-browser_en-US/\") as driver:\n",
    "    driver = visit_and_login(driver)\n",
    "    all_urls = get_pages(driver, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18aac11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the urls with links to vendor profiles\n",
    "vendor_urls = []\n",
    "\n",
    "for i in all_urls: #go to all scraped urls\n",
    "    if 'profile' in i and 'jheronimus' not in i: #do not save our own profile\n",
    "        vendor_urls.append(i)\n",
    "\n",
    "vendor_urls = list(dict.fromkeys(vendor_urls))#final list of vendor urls /remove duplicates\n",
    "vendor_urls = [URL[:-1]+ i for i in vendor_urls]\n",
    "\n",
    "import json\n",
    "with open(\"asap_urls.json\", \"w\") as f: #save urls as a json file\n",
    "    json.dump(vendor_urls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36075d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the crawling\n",
      "dowloaded 1 out of 173 pages\n",
      "dowloaded 2 out of 173 pages\n",
      "dowloaded 3 out of 173 pages\n",
      "dowloaded 4 out of 173 pages\n",
      "dowloaded 5 out of 173 pages\n",
      "dowloaded 6 out of 173 pages\n",
      "dowloaded 7 out of 173 pages\n",
      "dowloaded 8 out of 173 pages\n",
      "dowloaded 9 out of 173 pages\n",
      "dowloaded 10 out of 173 pages\n",
      "dowloaded 11 out of 173 pages\n",
      "dowloaded 12 out of 173 pages\n",
      "dowloaded 13 out of 173 pages\n",
      "dowloaded 14 out of 173 pages\n",
      "dowloaded 15 out of 173 pages\n",
      "dowloaded 16 out of 173 pages\n",
      "dowloaded 17 out of 173 pages\n",
      "dowloaded 18 out of 173 pages\n",
      "dowloaded 19 out of 173 pages\n",
      "dowloaded 20 out of 173 pages\n",
      "dowloaded 21 out of 173 pages\n",
      "dowloaded 22 out of 173 pages\n",
      "dowloaded 23 out of 173 pages\n",
      "dowloaded 24 out of 173 pages\n",
      "dowloaded 25 out of 173 pages\n",
      "dowloaded 26 out of 173 pages\n",
      "dowloaded 27 out of 173 pages\n",
      "dowloaded 28 out of 173 pages\n",
      "dowloaded 29 out of 173 pages\n",
      "dowloaded 30 out of 173 pages\n",
      "dowloaded 31 out of 173 pages\n",
      "dowloaded 32 out of 173 pages\n",
      "dowloaded 33 out of 173 pages\n",
      "dowloaded 34 out of 173 pages\n",
      "dowloaded 35 out of 173 pages\n",
      "dowloaded 36 out of 173 pages\n",
      "dowloaded 37 out of 173 pages\n",
      "dowloaded 38 out of 173 pages\n",
      "dowloaded 39 out of 173 pages\n",
      "finished crawling\n"
     ]
    }
   ],
   "source": [
    "#now visit all the urls we just scraped and download them\n",
    "\n",
    "def get_vendor_pages(driver, vendor_urls):\n",
    "    print(\"Starting the crawling\")\n",
    "  \n",
    "    for index, i in enumerate(vendor_urls): #for all vendors_urls\n",
    "        \n",
    "        site_page = i #name of url\n",
    "        driver.get(site_page) #visit page\n",
    "        time.sleep(2)\n",
    "        \n",
    "        #find the vendor name in the page\n",
    "        soup = bs(driver.page_source, 'html.parser')\n",
    "        name = soup.find_all(class_ = \"btn btn-sm btn-success\")\n",
    "        \n",
    "        vendor_name = name[0]['href'][15:]  #function to extract vendor name\n",
    "        \n",
    "        \n",
    "        filename = 'asap/'+ vendor_name + '.html' #save file als username\n",
    "        with open(\"/home/levi/\" + filename, \"w\") as f:\n",
    "            f.write(driver.page_source)\n",
    "            print('dowloaded' ,index+1, 'out of', len(vendor_urls),'pages') #print statement for progress\n",
    "    print('finished crawling')\n",
    "    \n",
    "with TorBrowserDriver(\"/home/levi/tor-browser_en-US/\") as driver:\n",
    "    driver = visit_and_login(driver)\n",
    "    get_vendor_pages(driver, vendor_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb753da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SpacePope',\n",
       " 'actioneu',\n",
       " 'TopShellNL',\n",
       " 'danielvitor61',\n",
       " 'mushmedic',\n",
       " 'rugbykonix',\n",
       " 'pharmatec',\n",
       " 'maurelius',\n",
       " 'ADHD',\n",
       " 'Hilfiger',\n",
       " 'lutonlu',\n",
       " 'ddprovider',\n",
       " 'incognitous',\n",
       " 'blackmarket',\n",
       " 'TheOxyMen',\n",
       " 'gwolf',\n",
       " 'sixandeight',\n",
       " 'Pegasus',\n",
       " 'Paradise420',\n",
       " 'DutchHope',\n",
       " 'DrWhit3',\n",
       " 'authenticity',\n",
       " 'klaasflakko',\n",
       " 'whitebeer',\n",
       " 'MrGrumpy',\n",
       " 'TorDrug',\n",
       " 'TheHappyDwarf',\n",
       " 'Lemmonflavor',\n",
       " 'topshopuk',\n",
       " 'DumpsTracks',\n",
       " 'DutchMagic',\n",
       " 'projeccao',\n",
       " 'bulkversion',\n",
       " 'superwave',\n",
       " 'ExpressService',\n",
       " 'onestopshop',\n",
       " 'Pygmalion',\n",
       " 'Pushar',\n",
       " 'mysteryland',\n",
       " 'kim0ra',\n",
       " 'cocokingz',\n",
       " 'PharmaFR',\n",
       " 'PSteroids',\n",
       " 'ezinpharma',\n",
       " 'oxytop',\n",
       " 'dmtking',\n",
       " 'HappyShopOrigi',\n",
       " 'procarder2',\n",
       " 'unitedpharma',\n",
       " 'SocialMedia',\n",
       " 'boomshankar',\n",
       " 'Narco24',\n",
       " 'TulipNL',\n",
       " 'DreamWeaver',\n",
       " 'Tornad0cc',\n",
       " 'craigman',\n",
       " 'PharmaWorldUK',\n",
       " 'BigPenis',\n",
       " 'PrescoxySquad',\n",
       " 'EUCarder',\n",
       " 'HighFly',\n",
       " 'PostNL',\n",
       " 'cmoney345',\n",
       " '8drugsafe',\n",
       " 'fraudbuddy',\n",
       " 'monoko',\n",
       " 'ICEGOD',\n",
       " 'HeartKidnapper',\n",
       " 'anonyps',\n",
       " 'JOYinc',\n",
       " 'GQDS24']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sometimes we get thrown of the website, we check which links we already visited and update the urls accordingly\n",
    "\n",
    "def links_to_crawl(DIR, vendor_urls):\n",
    "    '''check in directory which pages already have been crawled and make new list with urls to crawl'''\n",
    "    \n",
    "    crawled = os.listdir(DIR)\n",
    "    crawled = [i[:-5] for i in crawled]\n",
    "    \n",
    "    print('already crawled pages:' , len(crawled))\n",
    "\n",
    "    all_vendors = [i[89:] for i in vendor_urls] #names of all vendors\n",
    "\n",
    "    #all vendors remaining to crawl\n",
    "    to_crawl = [vendor for vendor in all_vendors if vendor not in crawled]\n",
    "    site = URL+'?page=profile&user='\n",
    "\n",
    "    vendors_to_crawl = [site + vendor for vendor in to_crawl] #list of pages to crawl\n",
    "    print('pages left to crawl' , len(vendors_to_crawl))\n",
    "    \n",
    "    return vendors_to_crawl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171b81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74fdd5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from tbselenium.tbdriver import TorBrowserDriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ea1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://asap4u7rq4tyakf5gdahmj2c77blwc4noxnsppp5lzlhk7x34x2e22yd.onion/'\n",
    "\n",
    "\n",
    "def visit_and_login(driver):\n",
    "    '''This function opens the marketplace in the url'''\n",
    "    \n",
    "    # Log in and solve the captcha\n",
    "    driver.get(URL)\n",
    "    time.sleep(30) #30 seconds to solve the captcha and login\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "def get_pages(driver, urls):\n",
    "    print(\"Starting the crawling\")\n",
    "    \n",
    "    for i in range(1, 239):\n",
    "        site_page = URL +'search?page='+str(i)+'&currency=USD&sortType=None'\n",
    "        driver.get(site_page)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        \n",
    "        soup = bs(driver.page_source, 'html.parser')\n",
    "        hrefs = soup.find_all(href=True) #find all linked urls\n",
    "        for line in hrefs:\n",
    "            urls.append(line['href'])\n",
    "        \n",
    "        if i%5 == 0:\n",
    "            print('crawled',i,'out of 239 pages')\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    print('Finished Crawling')\n",
    "    return urls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9f3da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e42cc7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the crawling\n",
      "crawled 5 out of 239 pages\n",
      "crawled 10 out of 239 pages\n",
      "crawled 15 out of 239 pages\n",
      "crawled 20 out of 239 pages\n",
      "crawled 25 out of 239 pages\n",
      "crawled 30 out of 239 pages\n",
      "crawled 35 out of 239 pages\n",
      "crawled 40 out of 239 pages\n",
      "crawled 45 out of 239 pages\n",
      "crawled 50 out of 239 pages\n",
      "crawled 55 out of 239 pages\n",
      "crawled 60 out of 239 pages\n",
      "crawled 65 out of 239 pages\n",
      "crawled 70 out of 239 pages\n",
      "crawled 75 out of 239 pages\n",
      "crawled 80 out of 239 pages\n",
      "crawled 85 out of 239 pages\n",
      "crawled 90 out of 239 pages\n",
      "crawled 95 out of 239 pages\n",
      "crawled 100 out of 239 pages\n",
      "crawled 105 out of 239 pages\n",
      "crawled 110 out of 239 pages\n",
      "crawled 115 out of 239 pages\n",
      "crawled 120 out of 239 pages\n",
      "crawled 125 out of 239 pages\n",
      "crawled 130 out of 239 pages\n",
      "crawled 135 out of 239 pages\n",
      "crawled 140 out of 239 pages\n",
      "crawled 145 out of 239 pages\n",
      "crawled 150 out of 239 pages\n",
      "crawled 155 out of 239 pages\n",
      "crawled 160 out of 239 pages\n",
      "crawled 165 out of 239 pages\n",
      "crawled 170 out of 239 pages\n",
      "crawled 175 out of 239 pages\n",
      "crawled 180 out of 239 pages\n",
      "crawled 185 out of 239 pages\n",
      "crawled 190 out of 239 pages\n",
      "crawled 195 out of 239 pages\n",
      "crawled 200 out of 239 pages\n",
      "crawled 205 out of 239 pages\n",
      "crawled 210 out of 239 pages\n",
      "crawled 215 out of 239 pages\n",
      "crawled 220 out of 239 pages\n",
      "crawled 225 out of 239 pages\n",
      "crawled 230 out of 239 pages\n",
      "crawled 235 out of 239 pages\n",
      "Finished Crawling\n"
     ]
    }
   ],
   "source": [
    "with TorBrowserDriver(\"/home/levi/tor-browser_en-US/\") as driver:\n",
    "    driver = visit_and_login(driver)\n",
    "    all_urls = get_pages(driver, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18aac11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor_urls = []\n",
    "for i in urls:\n",
    "    if 'profile' in i and 'jheronimus' not in i:\n",
    "        vendor_urls.append(i)\n",
    "\n",
    "vendor_urls = list(dict.fromkeys(vendor_urls))#final list of vendor urls /remove duplicates\n",
    "vendor_urls = [URL[:-1]+ i for i in vendor_urls]\n",
    "\n",
    "import json\n",
    "with open(\"asap_urls.json\", \"w\") as f:\n",
    "    json.dump(vendor_urls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36075d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the crawling\n",
      "dowloaded 1 out of 173 pages\n",
      "dowloaded 2 out of 173 pages\n",
      "dowloaded 3 out of 173 pages\n",
      "dowloaded 4 out of 173 pages\n",
      "dowloaded 5 out of 173 pages\n",
      "dowloaded 6 out of 173 pages\n",
      "dowloaded 7 out of 173 pages\n",
      "dowloaded 8 out of 173 pages\n",
      "dowloaded 9 out of 173 pages\n",
      "dowloaded 10 out of 173 pages\n",
      "dowloaded 11 out of 173 pages\n",
      "dowloaded 12 out of 173 pages\n",
      "dowloaded 13 out of 173 pages\n",
      "dowloaded 14 out of 173 pages\n",
      "dowloaded 15 out of 173 pages\n",
      "dowloaded 16 out of 173 pages\n",
      "dowloaded 17 out of 173 pages\n",
      "dowloaded 18 out of 173 pages\n",
      "dowloaded 19 out of 173 pages\n",
      "dowloaded 20 out of 173 pages\n",
      "dowloaded 21 out of 173 pages\n",
      "dowloaded 22 out of 173 pages\n",
      "dowloaded 23 out of 173 pages\n",
      "dowloaded 24 out of 173 pages\n",
      "dowloaded 25 out of 173 pages\n",
      "dowloaded 26 out of 173 pages\n",
      "dowloaded 27 out of 173 pages\n",
      "dowloaded 28 out of 173 pages\n",
      "dowloaded 29 out of 173 pages\n",
      "dowloaded 30 out of 173 pages\n",
      "dowloaded 31 out of 173 pages\n",
      "dowloaded 32 out of 173 pages\n",
      "dowloaded 33 out of 173 pages\n",
      "dowloaded 34 out of 173 pages\n",
      "dowloaded 35 out of 173 pages\n",
      "dowloaded 36 out of 173 pages\n",
      "dowloaded 37 out of 173 pages\n",
      "dowloaded 38 out of 173 pages\n",
      "dowloaded 39 out of 173 pages\n",
      "finished crawling\n"
     ]
    }
   ],
   "source": [
    "def get_vendor_pages(driver, vendor_urls):\n",
    "    print(\"Starting the crawling\")\n",
    "  \n",
    "    for index, i in enumerate(vendor_urls[134:]):\n",
    "        \n",
    "        site_page = i \n",
    "        driver.get(site_page)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        #find the vendor name in the page\n",
    "        soup = bs(driver.page_source, 'html.parser')\n",
    "        name = soup.find_all(class_ = \"btn btn-sm btn-success\")\n",
    "        \n",
    "        vendor_name = name[0]['href'][15:]  #function to extract vendor name\n",
    "        \n",
    "        \n",
    "        filename = 'asap/'+ vendor_name + '.html' #save file als username\n",
    "        with open(\"/home/levi/\" + filename, \"w\") as f:\n",
    "            f.write(driver.page_source)\n",
    "            print('dowloaded' ,index+1, 'out of', len(vendor_urls),'pages')\n",
    "    print('finished crawling')\n",
    "    \n",
    "with TorBrowserDriver(\"/home/levi/tor-browser_en-US/\") as driver:\n",
    "    driver = visit_and_login(driver)\n",
    "    get_vendor_pages(driver, vendor_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb753da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SpacePope',\n",
       " 'actioneu',\n",
       " 'TopShellNL',\n",
       " 'danielvitor61',\n",
       " 'mushmedic',\n",
       " 'rugbykonix',\n",
       " 'pharmatec',\n",
       " 'maurelius',\n",
       " 'ADHD',\n",
       " 'Hilfiger',\n",
       " 'lutonlu',\n",
       " 'ddprovider',\n",
       " 'incognitous',\n",
       " 'blackmarket',\n",
       " 'TheOxyMen',\n",
       " 'gwolf',\n",
       " 'sixandeight',\n",
       " 'Pegasus',\n",
       " 'Paradise420',\n",
       " 'DutchHope',\n",
       " 'DrWhit3',\n",
       " 'authenticity',\n",
       " 'klaasflakko',\n",
       " 'whitebeer',\n",
       " 'MrGrumpy',\n",
       " 'TorDrug',\n",
       " 'TheHappyDwarf',\n",
       " 'Lemmonflavor',\n",
       " 'topshopuk',\n",
       " 'DumpsTracks',\n",
       " 'DutchMagic',\n",
       " 'projeccao',\n",
       " 'bulkversion',\n",
       " 'superwave',\n",
       " 'ExpressService',\n",
       " 'onestopshop',\n",
       " 'Pygmalion',\n",
       " 'Pushar',\n",
       " 'mysteryland',\n",
       " 'kim0ra',\n",
       " 'cocokingz',\n",
       " 'PharmaFR',\n",
       " 'PSteroids',\n",
       " 'ezinpharma',\n",
       " 'oxytop',\n",
       " 'dmtking',\n",
       " 'HappyShopOrigi',\n",
       " 'procarder2',\n",
       " 'unitedpharma',\n",
       " 'SocialMedia',\n",
       " 'boomshankar',\n",
       " 'Narco24',\n",
       " 'TulipNL',\n",
       " 'DreamWeaver',\n",
       " 'Tornad0cc',\n",
       " 'craigman',\n",
       " 'PharmaWorldUK',\n",
       " 'BigPenis',\n",
       " 'PrescoxySquad',\n",
       " 'EUCarder',\n",
       " 'HighFly',\n",
       " 'PostNL',\n",
       " 'cmoney345',\n",
       " '8drugsafe',\n",
       " 'fraudbuddy',\n",
       " 'monoko',\n",
       " 'ICEGOD',\n",
       " 'HeartKidnapper',\n",
       " 'anonyps',\n",
       " 'JOYinc',\n",
       " 'GQDS24']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def links_to_crawl(DIR, vendor_urls):\n",
    "    '''check in directory which pages already have been crawled and make new list with urls to crawl'''\n",
    "    \n",
    "    crawled = os.listdir(DIR)\n",
    "    crawled = [i[:-5] for i in crawled]\n",
    "    \n",
    "    print('already crawled pages:' , len(crawled))\n",
    "\n",
    "    all_vendors = [i[89:] for i in vendor_urls] #names of all vendors\n",
    "\n",
    "    #all vendors remaining to crawl\n",
    "    to_crawl = [vendor for vendor in all_vendors if vendor not in crawled]\n",
    "    site = URL+'?page=profile&user='\n",
    "\n",
    "    vendors_to_crawl = [site + vendor for vendor in to_crawl] #list of pages to crawl\n",
    "    print('pages left to crawl' , len(vendors_to_crawl))\n",
    "    \n",
    "    return vendors_to_crawl\n",
    "\n",
    "\n",
    "DIR = r'/home/levi/asap'\n",
    "dir1 = os.listdir(DIR)\n",
    "asap = [i[:-5] for i in dir1]\n",
    "\n",
    "\n",
    "\n",
    "DIR2 =r'/home/levi/darkfox'\n",
    "dir2 = os.listdir(DIR2)\n",
    "vice = [i[:-5] for i in dir2]\n",
    "\n",
    "list(set(vice).intersection(asap))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171b81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
